{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.dummy import DummyClassifier\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer, sent_tokenize, word_tokenize\n",
    "from pprint import pprint as pp\n",
    "import string\n",
    "from nltk.stem.porter import PorterStemmer \n",
    "from collections import Counter \n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.corpora.dictionary import Dictionary \n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "import matplotlib\n",
    "import matplotlib.patches as mpatches\n",
    "import itertools\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "import warnings\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import torch\n",
    "import transformers as ppb\n",
    "from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler, TensorDataset)\n",
    "from torch.nn import CrossEntropyLoss, MSELoss\n",
    "\n",
    "from tqdm import tqdm_notebook, trange\n",
    "import os\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM, BertForSequenceClassification\n",
    "from pytorch_pretrained_bert.optimization import BertAdam, WarmupLinearSchedule\n",
    "from transformers import (glue_convert_examples_to_features, glue_output_modes,\n",
    "glue_processors)\n",
    "from multiprocessing import Pool, cpu_count\n",
    "#import tools \n",
    "#import convert_examples_to_features\n",
    "import joblib\n",
    "# OPTIONAL: if you want to have more information on what's happening, activate the logger as follows\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "import inflect\n",
    "from num2words import num2words\n",
    "import gensim\n",
    "warnings.simplefilter(\"ignore\", DeprecationWarning)\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "\n",
    "p = inflect.engine() \n",
    "\n",
    "def cv(data):\n",
    "    \n",
    "    count_vectorizer = CountVectorizer()\n",
    "    \n",
    "    emp = count_vectorizer.fit_transform(data)\n",
    "    \n",
    "    return emp, count_vectorizer\n",
    "# convert number into words \n",
    "def convert_number(text): \n",
    "    # split string into list of words \n",
    "    temp_str = text.split() \n",
    "    # initialise empty list \n",
    "    new_string = [] \n",
    "    \n",
    "    for word in temp_str: \n",
    "        # if word is a digit, convert the digit \n",
    "        # to numbers and append into the new_string list \n",
    "        if word.isdigit(): \n",
    "            temp = p.number_to_words(word) \n",
    "            new_string.append(temp) \n",
    "  # append the word as it is \n",
    "        else: \n",
    "            new_string.append(word) \n",
    "# join the words of new_string to form a string \n",
    "    temp_str = ' '.join(new_string) \n",
    "    return temp_str \n",
    "  \n",
    "\n",
    "def standardize_text(df, text_field):\n",
    "    \"\"\" Function for cleaning text\"\"\"\n",
    "    df[text_field] = df[text_field].str.replace(r\"http\\S+\", \"\")\n",
    "    df[text_field] = df[text_field].str.replace(r\"http\", \"\")\n",
    "    df[text_field] = df[text_field].str.replace(r\"@\\S+\", \"\")\n",
    "    df[text_field] = df[text_field].str.replace(r\"[^A-Za-z0-9()!?@\\'\\`\\\"\\_\\n]\", \" \")\n",
    "    df[text_field] = df[text_field].str.replace(r\"@\", \"at\")\n",
    "    df[text_field] = df[text_field].str.replace(r\"'\", \"\")\n",
    "    df[text_field] = df[text_field].str.replace(r\"!\", \"\") \n",
    "    df[text_field] = df[text_field].str.lower()\n",
    "    return df\n",
    "\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    return [lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(text)]\n",
    "\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.winter):\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title, fontsize=30)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, fontsize=20)\n",
    "    plt.yticks(tick_marks, classes, fontsize=20)\n",
    "    \n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt), horizontalalignment=\"center\", \n",
    "                 color=\"white\" if cm[i, j] < thresh else \"black\", fontsize=40)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label', fontsize=30)\n",
    "    plt.xlabel('Predicted label', fontsize=30)\n",
    "\n",
    "    return plt\n",
    "\n",
    "def converttostr(input_seq, seperator):\n",
    "    final_str = seperator.join(input_seq)   \n",
    "    return final_str\n",
    "\n",
    "def print_topics(model, count_vectorizer, n_top_words):\n",
    "    words = count_vectorizer.get_feature_names()\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"\\nTopic #%d:\" % topic_idx)\n",
    "        print(\" \".join([words[i]\n",
    "                        for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "        \n",
    "def cv(data):\n",
    "    \n",
    "    count_vectorizer = CountVectorizer()\n",
    "    \n",
    "    emp = count_vectorizer.fit_transform(data)\n",
    "    \n",
    "    return emp, count_vectorizer\n",
    "\n",
    "def tfidf(data):\n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "    train = tfidf_vectorizer.fit_transform(data)\n",
    "\n",
    "    return train, tfidf_vectorizer\n",
    "\n",
    "\n",
    "def remove_stopwords(text): \n",
    "    stop_words = set(stopwords.words(\"english\")) \n",
    "    word_tokens = word_tokenize(text) \n",
    "    filtered_text = [word for word in word_tokens if word not in stop_words] \n",
    "    return filtered_text \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.tokenize import word_tokenize \n",
    "lemmatizer = WordNetLemmatizer() \n",
    "# lemmatize string \n",
    "def lemmatize_word(text): \n",
    "    word_tokens = word_tokenize(text) \n",
    "    lemmas = [lemmatizer.lemmatize(word, pos ='v') for word in word_tokens] \n",
    "    return lemmas \n",
    "def get_metrics(y_test, y_predicted):  \n",
    "    # true positives / (true positives+false positives)\n",
    "    precision = precision_score(y_test, y_predicted, pos_label=None,\n",
    "                                    average='weighted')             \n",
    "    # true positives / (true positives + false negatives)\n",
    "    recall = recall_score(y_test, y_predicted, pos_label=None,\n",
    "                              average='weighted')\n",
    "    \n",
    "    # harmonic mean of precision and recall\n",
    "    f1 = f1_score(y_test, y_predicted, pos_label=None, average='weighted')\n",
    "    \n",
    "    # true positives + true negatives/ total\n",
    "    accuracy = accuracy_score(y_test, y_predicted)\n",
    "    return accuracy, precision, recall, f1\n",
    "def plot_important_words(top_scores, top_words, bottom_scores, bottom_words, name):\n",
    "    y_pos = np.arange(len(top_words))\n",
    "    top_pairs = [(a,b) for a,b in zip(top_words, top_scores)]\n",
    "    top_pairs = sorted(top_pairs, key=lambda x: x[1])\n",
    "    \n",
    "    bottom_pairs = [(a,b) for a,b in zip(bottom_words, bottom_scores)]\n",
    "    bottom_pairs = sorted(bottom_pairs, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    top_words = [a[0] for a in top_pairs]\n",
    "    top_scores = [a[1] for a in top_pairs]\n",
    "    \n",
    "    bottom_words = [a[0] for a in bottom_pairs]\n",
    "    bottom_scores = [a[1] for a in bottom_pairs]\n",
    "    \n",
    "    fig = plt.figure(figsize=(10, 10))  \n",
    "\n",
    "    plt.subplot(121)\n",
    "    plt.barh(y_pos,bottom_scores, align='center', alpha=0.5)\n",
    "    plt.title('Neutral', fontsize=20)\n",
    "    plt.yticks(y_pos, bottom_words, fontsize=14)\n",
    "    plt.suptitle('Key words', fontsize=16)\n",
    "    plt.xlabel('Importance', fontsize=20)\n",
    "    \n",
    "    plt.subplot(122)\n",
    "    plt.barh(y_pos,top_scores, align='center', alpha=0.5)\n",
    "    plt.title('Disaster', fontsize=20)\n",
    "    plt.yticks(y_pos, top_words, fontsize=14)\n",
    "    plt.suptitle(name, fontsize=16)\n",
    "    plt.xlabel('Importance', fontsize=20)\n",
    "    \n",
    "    plt.subplots_adjust(wspace=0.8)\n",
    "    plt.show()\n",
    "#https://github.com/hundredblocks/concrete_NLP_tutorial/blob/master/NLP_notebook.ipynb\n",
    "def get_most_important_features(vectorizer, model, n = 5):\n",
    "    index_to_word = {v:k for k,v in vectorizer.vocabulary_.items()}\n",
    "    \n",
    "    #Loop for each class\n",
    "    classes = {}\n",
    "    for class_index in range(model.coef_.shape[0]):\n",
    "        word_importances = [(el, index_to_word[i]) for i,el in enumerate(model.coef_[class_index])]\n",
    "        sorted_coeff = sorted(word_importances, key = lambda x : x[0], reverse=True)\n",
    "        tops = sorted(sorted_coeff[:n], key = lambda x : x[0])\n",
    "        bottom = sorted_coeff[-n:]\n",
    "        classes[class_index] = {\n",
    "            'tops':tops,\n",
    "            'bottom':bottom\n",
    "        }\n",
    "    return classes\n",
    "def word_feats(words):\n",
    "        return dict([(word, True) for word in words])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>cat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"dangerous \" around world police chokeholds sc...</td>\n",
       "      <td>0</td>\n",
       "      <td>gen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"its been hectic\" americans struggle as unempl...</td>\n",
       "      <td>0</td>\n",
       "      <td>gen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"its been hectic\" americans struggle as unempl...</td>\n",
       "      <td>0</td>\n",
       "      <td>gen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"people just dont want to be killed period \" a...</td>\n",
       "      <td>2</td>\n",
       "      <td>gen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"the last dance\" episodes nine ten recap and s...</td>\n",
       "      <td>2</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   x  y     cat\n",
       "0  \"dangerous \" around world police chokeholds sc...  0     gen\n",
       "1  \"its been hectic\" americans struggle as unempl...  0     gen\n",
       "2  \"its been hectic\" americans struggle as unempl...  0     gen\n",
       "3  \"people just dont want to be killed period \" a...  2     gen\n",
       "4  \"the last dance\" episodes nine ten recap and s...  2  sports"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('black_news_sent.csv')\n",
    "df.drop(['Unnamed: 0'], axis = 1, inplace = True)\n",
    "\n",
    "df.Sentiment[df.Sentiment == 'Negative'] = 0\n",
    "df.Sentiment[df.Sentiment == 'Neutral'] = 1\n",
    "df.Sentiment[df.Sentiment == 'Positive'] = 2\n",
    "\n",
    "df.dropna(inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "sent = df.Sentiment\n",
    "\n",
    "tokenizer = nltk.RegexpTokenizer(r\"\\w+\") #Creat tokenizer\n",
    "df = standardize_text(df, 'newtitle') #remove punctuation and lower case\n",
    "\n",
    "title = standardize_text(df, 'newtitle') #remove punctuation and lower case\n",
    "\n",
    "title = title.newtitle #grab titles\n",
    "ntitle = title.apply(convert_number) #Convert digits to alpha numbers\n",
    "stop_words = set(stopwords.words('english')) #Filter stop words\n",
    "words = [w for w in ntitle if not w in ntitle]\n",
    "words = pd.Series(words)\n",
    "new_words = words.apply(tokenizer.tokenize) #Tokenize\n",
    "new_words\n",
    "cats = df.cat\n",
    "df = pd.DataFrame({'x':words,'y': sent, 'cat': cats})\n",
    "df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8156736 words total, with a vocabulary size of 2854\n",
      "Max title length is 233\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "all_words = [word for tokens in df['x'] for word in words]\n",
    "title_lengths = [len(tokens) for tokens in df.x]\n",
    "VOCAB = sorted(list(set(all_words)))\n",
    "print(\"%s words total, with a vocabulary size of %s\" % (len(all_words), len(VOCAB)))\n",
    "print(\"Max title length is %s\" % max(title_lengths))\n",
    "fig = plt.figure(figsize = (5,5))\n",
    "plt.xlabel('Headline length')\n",
    "plt.ylabel('Number of headlines')\n",
    "plt.hist(title_lengths);\n",
    "plt.savefig('dist_headlines.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFIDF Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Generating Train_test_split and creating list of labels\n",
    "\n",
    "\n",
    "newcorpus = new_words.apply(lambda x: converttostr(x, seperator= ' '))\n",
    "list_corpus = newcorpus.tolist() \n",
    "\n",
    "label1=df['cat']\n",
    "label2 =  [0 if x == 'biz' else 1 if x == 'gen' else 2 for x in label1]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "X_train, X_test, y_train_cat, y_test_cat = train_test_split(list_corpus, label2, \n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=7274)\n",
    "\n",
    "X_train, tfidf_vectorizer = tfidf(X_train)\n",
    "X_test = tfidf_vectorizer.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:   22.3s\n",
      "[Parallel(n_jobs=-1)]: Done 138 tasks      | elapsed:  2.6min\n",
      "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed:  5.3min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, estimator=RandomForestClassifier(), n_iter=100,\n",
       "                   n_jobs=-1,\n",
       "                   param_distributions={'bootstrap': [True, False],\n",
       "                                        'max_depth': [10, 21, 32, 44, 55, 67,\n",
       "                                                      78, 90, None],\n",
       "                                        'max_features': ['auto', 'sqrt'],\n",
       "                                        'min_samples_leaf': [1, 2, 4],\n",
       "                                        'min_samples_split': [2, 5, 10],\n",
       "                                        'n_estimators': [200, 400, 600, 800,\n",
       "                                                         1000]},\n",
       "                   random_state=1984, scoring='precision_micro', verbose=2)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Random forest Randomized Search CV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "#n_estimators = max trees\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 1000, num = 5)]\n",
    "#Max_features = max # of features considered at each split\n",
    "max_features = ['auto', 'sqrt']\n",
    "#max_depth = max #of levels in each decision tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 90, num = 8)]\n",
    "max_depth.append(None)\n",
    "#Min_samples_split = min number of data points placed in a ndoe before node is split\n",
    "min_samples_split = [2, 5, 10]\n",
    "#min_samples_leaf= min number of data ponits allowed in a leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "#bootstrap = method for sampling data points (with or without replacement)\n",
    "bootstrap = [True,False]\n",
    "\n",
    "random_grid = {'n_estimators' : n_estimators,\n",
    "              'max_features': max_features,\n",
    "              'max_depth': max_depth,\n",
    "              'min_samples_split': min_samples_split,\n",
    "              'min_samples_leaf': min_samples_leaf,\n",
    "              'bootstrap': bootstrap}\n",
    "\n",
    "#Use random grid to search for best parameters\n",
    "rf_tfidf = RandomForestClassifier()\n",
    "\n",
    "\n",
    "#Random search of parameters using 3 fold cross validation, seaerch across 100 different combinations\n",
    "#Use all cores\n",
    "\n",
    "rf_tfidf = RandomizedSearchCV(estimator = rf_tfidf, param_distributions= random_grid,\n",
    "                              n_iter=100, cv = 3, verbose = 2, random_state=1984,\n",
    "                              n_jobs = -1 , scoring = 'precision_micro' )\n",
    "rf_tfidf.fit(X_train, y_train_cat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rf_tfidf.best_params_\n",
    "param_grid = {'n_estimators': [550, 575, 600, 625, 650],\n",
    " 'min_samples_split': [1,2,3],\n",
    " 'min_samples_leaf': [3,4,5],\n",
    " 'max_features': ['auto'],\n",
    " 'max_depth': [18,19,20,21,22,23,24],\n",
    " 'bootstrap': [False]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grid search RF \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Create a based model\n",
    "rf_tfidf = RandomForestClassifier()\n",
    "# Instantiate the grid search model\n",
    "rf_tfidf_grid = GridSearchCV(estimator = rf_tfidf, param_grid = param_grid, \n",
    "                          cv = 3, n_jobs = -1, verbose = 2 , scoring = 'precision_micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 315 candidates, totalling 945 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:    8.0s\n",
      "[Parallel(n_jobs=-1)]: Done 138 tasks      | elapsed:   47.2s\n",
      "[Parallel(n_jobs=-1)]: Done 341 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=-1)]: Done 624 tasks      | elapsed:  4.2min\n",
      "[Parallel(n_jobs=-1)]: Done 945 out of 945 | elapsed:  6.6min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, estimator=RandomForestClassifier(), n_jobs=-1,\n",
       "             param_grid={'bootstrap': [False],\n",
       "                         'max_depth': [18, 19, 20, 21, 22, 23, 24],\n",
       "                         'max_features': ['auto'],\n",
       "                         'min_samples_leaf': [3, 4, 5],\n",
       "                         'min_samples_split': [1, 2, 3],\n",
       "                         'n_estimators': [550, 575, 600, 625, 650]},\n",
       "             scoring='precision_micro', verbose=2)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_tfidf_grid.fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_tfidf= rf_tfidf_grid.best_estimator_\n",
    "#rf_tfidf.get_params()\n",
    "#'bootstrap': False,\n",
    "#  'ccp_alpha': 0.0,\n",
    "#  'class_weight': None,\n",
    "#  'criterion': 'gini',\n",
    "#  'max_depth': 18,\n",
    "#  'max_features': 'auto',\n",
    "#  'max_leaf_nodes': None,\n",
    "#  'max_samples': None,\n",
    "#  'min_impurity_decrease': 0.0,\n",
    "#  'min_impurity_split': None,\n",
    "#  'min_samples_leaf': 3,\n",
    "#  'min_samples_split': 2,\n",
    "#  'min_weight_fraction_leaf': 0.0,\n",
    "#  'n_estimators': 550,\n",
    "#  'n_jobs': None,\n",
    "#  'oob_score': False,\n",
    "#  'random_state': None,\n",
    "#  'verbose': 0,\n",
    "#  'warm_start': False}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic regression\n",
    "\n",
    "lg = LogisticRegression()\n",
    "\n",
    "penalties = ['l1', 'l2', 'elasticnet']\n",
    "C_range = [0.001, 0.01, 0.1, 1, 10, 100]\n",
    "iters = [int(x) for x in np.linspace(start = 200, stop = 1000, num = 5)]\n",
    "random_grid = {'penalty' : penalties,\n",
    "              'C': C_range,\n",
    "              'max_iter': iters\n",
    "              }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\camer\\anaconda3\\envs\\insight\\lib\\site-packages\\sklearn\\model_selection\\_search.py:278: UserWarning: The total space of parameters 90 is smaller than n_iter=100. Running 90 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  19 tasks      | elapsed:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 90 candidates, totalling 270 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 270 out of 270 | elapsed:    3.6s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, estimator=LogisticRegression(), n_iter=100, n_jobs=-1,\n",
       "                   param_distributions={'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
       "                                        'max_iter': [200, 400, 600, 800, 1000],\n",
       "                                        'penalty': ['l1', 'l2', 'elasticnet']},\n",
       "                   random_state=1984, scoring='precision_micro', verbose=2)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgrandomized = RandomizedSearchCV(estimator = lg, param_distributions= random_grid,\n",
    "                              n_iter=100, cv = 3, verbose = 2, random_state=1984,\n",
    "                              n_jobs = -1 , scoring = 'precision_micro' )\n",
    "lgrandomized.fit(X_train, y_train_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgrandomized.best_params_\n",
    "param_grid = {'penalty': ['l2'], \n",
    "              'max_iter': [100, 150, 200, 250, 300],\n",
    "              'C': [0.001, 0.002, 0.003]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Create a based model\n",
    "lg_tfidf = LogisticRegression()\n",
    "# Instantiate the grid search model\n",
    "lg_tfidf_grid = GridSearchCV(estimator = lg_tfidf, param_grid = param_grid, \n",
    "                          cv = 3, n_jobs = -1, verbose = 2 , scoring = 'precision_micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 15 candidates, totalling 45 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Done  45 out of  45 | elapsed:    0.6s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  45 out of  45 | elapsed:    0.6s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, estimator=LogisticRegression(), n_jobs=-1,\n",
       "             param_grid={'C': [0.001, 0.002, 0.003],\n",
       "                         'max_iter': [100, 150, 200, 250, 300],\n",
       "                         'penalty': ['l2']},\n",
       "             scoring='precision_micro', verbose=2)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lg_tfidf_grid.fit(X_train, y_train_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "lg_tfidf = lg_tfidf_grid.best_estimator_\n",
    "#lg_tfidf.get_params()\n",
    "# {'C': 0.001,\n",
    "#  'class_weight': None,\n",
    "#  'dual': False,\n",
    "#  'fit_intercept': True,\n",
    "#  'intercept_scaling': 1,\n",
    "#  'l1_ratio': None,\n",
    "#  'max_iter': 100,\n",
    "#  'multi_class': 'auto',\n",
    "#  'n_jobs': None,\n",
    "#  'penalty': 'l2',\n",
    "#  'random_state': None,\n",
    "#  'solver': 'lbfgs',\n",
    "#  'tol': 0.0001,\n",
    "#  'verbose': 0,\n",
    "#  'warm_start': False}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\camer\\anaconda3\\envs\\insight\\lib\\site-packages\\sklearn\\dummy.py:131: FutureWarning: The default value of strategy will change from stratified to prior in 0.24.\n",
      "  warnings.warn(\"The default value of strategy will change from \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DummyClassifier(random_state=231)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Dummy classifier\n",
    "dummy_mf = DummyClassifier(strategy=\"most_frequent\", random_state=231)\n",
    "dummy_mf.fit(X_train, y_train_cat)\n",
    "dummy = DummyClassifier(random_state=231)\n",
    "dummy.fit(X_train, y_train_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.94310722, 0.94310722, 0.94091904, 0.94091904, 0.94298246])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Cross evaluation compare\n",
    "from sklearn import model_selection\n",
    "logtfcv = model_selection.cross_val_score(lg_tfidf,X_train,y_train_cat,\n",
    "                                        scoring = 'precision_micro',\n",
    "                                         )\n",
    "logtfcv\n",
    "\n",
    "rftfcv = model_selection.cross_val_score(rf_tfidf,X_train,y_train_cat,\n",
    "                                        scoring = 'precision_micro',\n",
    "                                         )\n",
    "rftfcv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\camer\\anaconda3\\envs\\insight\\lib\\site-packages\\sklearn\\dummy.py:131: FutureWarning: The default value of strategy will change from stratified to prior in 0.24.\n",
      "  warnings.warn(\"The default value of strategy will change from \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.87964989, 0.88402626, 0.87964989, 0.87964989, 0.88157895])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dumm cross eval\n",
    "dummymfcv = model_selection.cross_val_score(dummy_mf,X_train,y_train_cat,\n",
    "                                        scoring = 'precision_micro',\n",
    "                                         )\n",
    "dummymfcv\n",
    "\n",
    "dummycv = model_selection.cross_val_score(dummy,X_train,y_train_cat,\n",
    "                                        scoring = 'precision_micro',\n",
    "                                         )\n",
    "dummycv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(                                                      x\n",
       " 0     trump launches defensive twitter spree as amer...\n",
       " 1     trudeau attends ant racism rally and takes a knee\n",
       " 2     dc mayor inaugurates black lives matter plaza ...\n",
       " 3     teaching for black lives a handbook to fight a...\n",
       " 4     how a call to police over a twenty bill unleas...\n",
       " ...                                                 ...\n",
       " 2279  carroll foy formally launches bid for virginia...\n",
       " 2280  joe biden needs black voters to win the presid...\n",
       " 2281  news24 com donald trump slams drew brees apolo...\n",
       " 2282  news24 com watch trump sparks controversy sayi...\n",
       " 2283  news24 com bayern munich show support for blac...\n",
       " \n",
       " [2284 rows x 1 columns],\n",
       " 0       \"dangerous \" around world police chokeholds sc...\n",
       " 1       \"its been hectic\" americans struggle as unempl...\n",
       " 2       \"its been hectic\" americans struggle as unempl...\n",
       " 3       \"people just dont want to be killed period \" a...\n",
       " 4       \"the last dance\" episodes nine ten recap and s...\n",
       "                               ...                        \n",
       " 2851    your pain is my pain global anti racism protes...\n",
       " 2852    zimbabwe summons us ambassador over national s...\n",
       " 2853    zimbabwe summons us envoy over george floyd pr...\n",
       " 2854    zuckerberg s cheap labor group defend black lives\n",
       " 2855    zuckerberg facebook leaving up trumps shooting...\n",
       " Name: x, Length: 2856, dtype: object)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train_cat, y_test_cat = train_test_split(list_corpus, label2, \n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=7274)\n",
    "X_train = pd.DataFrame({'x': X_train})\n",
    "\n",
    "X_train, df.x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at C:\\Users\\camer/.cache\\torch\\transformers\\26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/distilbert-base-uncased-config.json from cache at C:\\Users\\camer/.cache\\torch\\transformers\\a41e817d5c0743e29e86ff85edc8c257e61bc8d88e4271bb1b243b6e7614c633.8949e27aafafa845a18d98a0e3a88bc2d248bbc32a1b75947366664658f23b1c\n",
      "INFO:transformers.configuration_utils:Model config DistilBertConfig {\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/distilbert-base-uncased-pytorch_model.bin from cache at C:\\Users\\camer/.cache\\torch\\transformers\\ae9df7a8d658c4f3e1917a471a8a21cf678fa1d4cb91e7702dfe0598dbdcf354.c2015533705b9dff680ae707e205a35e2860e8d148b45d35085419d74fe57ac5\n",
      "INFO:transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at C:\\Users\\camer/.cache\\torch\\transformers\\26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/distilbert-base-uncased-config.json from cache at C:\\Users\\camer/.cache\\torch\\transformers\\a41e817d5c0743e29e86ff85edc8c257e61bc8d88e4271bb1b243b6e7614c633.8949e27aafafa845a18d98a0e3a88bc2d248bbc32a1b75947366664658f23b1c\n",
      "INFO:transformers.configuration_utils:Model config DistilBertConfig {\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/distilbert-base-uncased-pytorch_model.bin from cache at C:\\Users\\camer/.cache\\torch\\transformers\\ae9df7a8d658c4f3e1917a471a8a21cf678fa1d4cb91e7702dfe0598dbdcf354.c2015533705b9dff680ae707e205a35e2860e8d148b45d35085419d74fe57ac5\n"
     ]
    }
   ],
   "source": [
    "#First trying DistilBERT\n",
    "model_class, tokenizer_class, pretrained_weights = (ppb.DistilBertModel, ppb.DistilBertTokenizer, 'distilbert-base-uncased')\n",
    "\n",
    "## Want BERT instead of distilBERT? Uncomment the following line:\n",
    "#model_class, tokenizer_class, pretrained_weights = (ppb.BertModel, ppb.BertTokenizer, 'bert-base-uncased')\n",
    "\n",
    "# Load pretrained model/tokenizer\n",
    "tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
    "model = model_class.from_pretrained(pretrained_weights)\n",
    "tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
    "model = model_class.from_pretrained(pretrained_weights)\n",
    "tokenized = X_train['x'].apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 0\n",
    "for i in tokenized.values:\n",
    "    if len(i) > max_len:\n",
    "        max_len = len(i)\n",
    "\n",
    "padded = np.array([i + [0]*(max_len-len(i)) for i in tokenized.values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_mask = np.where(np.array(padded)!=0,1,0)\n",
    "\n",
    "attention_mask.shape\n",
    "input_ids = torch.tensor(padded)\n",
    "attention_mask = torch.tensor(attention_mask)\n",
    "input_ids = input_ids.clone().detach().to(torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#input_ids = torch.tensor(input_ids).to(torch.int64)\n",
    "\n",
    "with torch.no_grad():\n",
    "    print('...')\n",
    "    last_hidden_states = model(input_ids, attention_mask=attention_mask)\n",
    "x_train_features = last_hidden_states[0][:,0,:].numpy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random forest randomized cv\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "#n_estimators = max trees\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 1000, num = 5)]\n",
    "#Max_features = max # of features considered at each split\n",
    "max_features = ['auto', 'sqrt']\n",
    "#max_depth = max #of levels in each decision tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 90, num = 8)]\n",
    "max_depth.append(None)\n",
    "#Min_samples_split = min number of data points placed in a ndoe before node is split\n",
    "min_samples_split = [2, 5, 10]\n",
    "#min_samples_leaf= min number of data ponits allowed in a leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "#bootstrap = method for sampling data points (with or without replacement)\n",
    "bootstrap = [True,False]\n",
    "\n",
    "random_grid = {'n_estimators' : n_estimators,\n",
    "              'max_features': max_features,\n",
    "              'max_depth': max_depth,\n",
    "              'min_samples_split': min_samples_split,\n",
    "              'min_samples_leaf': min_samples_leaf,\n",
    "              'bootstrap': bootstrap}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_bert = RandomForestClassifier()\n",
    "\n",
    "\n",
    "#Random search of parameters using 3 fold cross validation, seaerch across 100 different combinations\n",
    "#Use all cores\n",
    "\n",
    "rf_bert = RandomizedSearchCV(estimator = rf_bert, param_distributions= random_grid,\n",
    "                              n_iter=100, cv = 3, verbose = 2, random_state=1984,\n",
    "                              n_jobs = -1 , scoring = 'precision_micro' )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:  2.9min\n"
     ]
    }
   ],
   "source": [
    "rf_bert.fit(x_train_features, y_train_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = \n",
    "rf_bert.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a based model\n",
    "rf_bert = RandomForestClassifier()\n",
    "# Instantiate the grid search model\n",
    "rf_bert_grid = GridSearchCV(estimator = rf_bert, param_grid = param_grid, \n",
    "                          cv = 3, n_jobs = -1, verbose = 2 , scoring = 'precision_micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_bert_grid.fit(x_train_features, y_train_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_bert = rf_bert_grid.best_estimator_\n",
    "rf_bert.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic regression\n",
    "\n",
    "lg = LogisticRegression()\n",
    "\n",
    "penalties = ['l1', 'l2', 'elasticnet']\n",
    "C_range = [0.001, 0.01, 0.1, 1, 10, 100]\n",
    "iters = [int(x) for x in np.linspace(start = 200, stop = 1000, num = 5)]\n",
    "random_grid = {'penalty' : penalties,\n",
    "              'C': C_range,\n",
    "              'max_iter': iters\n",
    "              }\n",
    "lgrandomized = RandomizedSearchCV(estimator = lg, param_distributions= random_grid,\n",
    "                              n_iter=100, cv = 3, verbose = 2, random_state=1984,\n",
    "                              n_jobs = -1 , scoring = 'precision_micro' )\n",
    "lgrandomized.fit(x_train_features, y_train_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = \n",
    "lgrandomized.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a based model\n",
    "lg_bert = RandomForestClassifier()\n",
    "# Instantiate the grid search model\n",
    "lg_bert_grid = GridSearchCV(estimator = lg_bert, param_grid = param_grid, \n",
    "                          cv = 3, n_jobs = -1, verbose = 2 , scoring = 'precision_micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lg_bert_grid.fit(x_train_features, y_train_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lg_bert = lg_bert_grid.best_estimator\n",
    "lg_bert.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_labels = sent.tolist()\n",
    "labels = df['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "insight",
   "language": "python",
   "name": "insight"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
